{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26091c2",
   "metadata": {},
   "source": [
    "## Understanding how to generate reponses using Ollama LLMs\n",
    "\n",
    "#### 1. Intuition: The \"Open-Book Exam\"  \n",
    "Imagine you are a brilliant student (the LLM), but you have amnesia.  \n",
    "You remember everything up to 2023, but nothing after that.  \n",
    "\n",
    "- **Without RAG:**  \n",
    "  If I ask you about news from today, you might panic and make something up to sound smart.  \n",
    "  This is called **Hallucination**.  \n",
    "\n",
    "- **With RAG:**  \n",
    "  Before you answer, I hand you a newspaper from today.  \n",
    "  You read the specific article relevant to my question, and then you answer using that information.  \n",
    "\n",
    "**RAG is simply the process of handing the LLM the right notes before it speaks.**\n",
    "\n",
    "#### 2. Visual-in-Words: The Architecture  \n",
    "Close your eyes and visualize this flow:  \n",
    "\n",
    "1. The **User** asks a question:  \n",
    "   *\"How do I reset the X-2000 router?\"*  \n",
    "\n",
    "2. The **Retriever** (The Hand) reaches into a bucket of manuals (your database), rummages around, and grabs the one specific paragraph about resetting the X-2000.  \n",
    "\n",
    "3. The **Augmenter** (The Glue) takes the User's question and tapes that paragraph to it.  \n",
    "   New Prompt:  \n",
    "   *\"Context: [Reset paragraph]. Question: How do I reset the X-2000 router?\"*  \n",
    "\n",
    "4. The **Generator** (The Brain / ollama) reads the combined text and generates an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0b202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Answer: The capital of India is **New Delhi**. \n",
      "\n",
      "It's a planned city, officially named New Delhi, though the larger metropolitan area is often referred to as Delhi. ðŸ˜Š \n",
      "\n",
      "Do you want to know anything more about New Delhi or India in general?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# 1. Define the question\n",
    "user_question = \"What is the capital of India?\"\n",
    "\n",
    "# 2. Send it to the model (Generation only)\n",
    "# We are NOT providing external context yet.\n",
    "response = ollama.chat(\n",
    "    model='gemma3:4b',  # Enter the model that you pull on ollama\n",
    "    messages=[{'role': 'user', 'content': user_question}]\n",
    ")\n",
    "\n",
    "# 3. Print the answer\n",
    "print(\"AI Answer:\", response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0e749",
   "metadata": {},
   "source": [
    "We write,<br>\n",
    "`response['message']['content']`<br>\n",
    "because it is in JSON format:<br>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model\": \"phi3:3.8b\",\n",
    "  \"created_at\": \"2025-12-15T09:45:00.5105742Z\",\n",
    "  \"done\": true,\n",
    "  \"done_reason\": \"stop\",\n",
    "  \"total_duration\": 11129766800,\n",
    "  \"load_duration\": 3728855400,\n",
    "  \"prompt_eval_count\": 16,\n",
    "  \"prompt_eval_duration\": 765201100,\n",
    "  \"eval_count\": 94,\n",
    "  \"eval_duration\": 6592721000,\n",
    "  \"message\": {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"The capital of India is New Delhi. It became the political center in 1956 when the country's government moved from Kolkata (then called Calcutta) and now serves as a major hub for culture, education, and politics, housing Parliament House, which holds the Indian legislative bodies - Lok Sabha (House of the People), Rajya Sabha (Council of States), along with numerous other government offices.\",\n",
    "    \"thinking\": null,\n",
    "    \"images\": null,\n",
    "    \"tool_name\": null,\n",
    "    \"tool_calls\": null\n",
    "  },\n",
    "  \"logprobs\": null\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8660b",
   "metadata": {},
   "source": [
    "### Different Roles\n",
    "\n",
    "| Role      | Analogy               | Purpose                                  |\n",
    "|-----------|-----------------------|------------------------------------------|\n",
    "| System    | The Boss / Director   | Defines behavior, tone, and rules.        |\n",
    "| User      | The Customer          | Asks the question.                       |\n",
    "| Assistant | The Transcript        | Provides conversation history/context.   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a01c85",
   "metadata": {},
   "source": [
    "```py\n",
    "messages=[\n",
    "    # 1. We set the context (optional but good)\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "\n",
    "    # 2. First turn\n",
    "    {'role': 'user', 'content': 'Who wrote Harry Potter?'},\n",
    "    {'role': 'assistant', 'content': 'J.K. Rowling wrote it.'}, # <--- WE INSERT THIS MANUALLY OR FROM PREVIOUS RESPONSE\n",
    "\n",
    "    # 3. Current question (The AI now reads the line above and knows 'she' = Rowling)\n",
    "    {'role': 'user', 'content': 'What year was she born?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3bcef",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "#### 1. Intuition: The Supermarket Aisle  \n",
    "Imagine you go to a supermarket and ask a clerk: â€œI need ingredients for a salad.â€  \n",
    "\n",
    "- **Keyword Search (The old Way):**  \n",
    "The clerk looks for items that have the word â€œSaladâ€ written on them.  \n",
    "They hand you â€œSalad Dressingâ€ and â€œSalad Tongsâ€.  \n",
    "They miss the Lettuce and Tomatoes because those items donâ€™t have the word â€œSaladâ€ printed on them.  \n",
    "\n",
    "- **Vector Search (Embeddings â€“ The RAG Way):**  \n",
    "The clerk understands the concept of a salad.  \n",
    "They know that Lettuce, Tomatoes, and Cucumbers â€œliveâ€ in the same conceptual aisle as â€œSaladâ€, even if the words are different.  \n",
    "\n",
    "Embeddings turn text into a coordinate on a map.  \n",
    "Words with similar meanings live close together.\n",
    "\n",
    "#### 2. Visual-in-Words: The 3-D Map  \n",
    "Imagine a giant 3-D box floating in space.  \n",
    "Every sentence you can possibly speak is a tiny dot inside this box.  \n",
    "\n",
    "- The dot for â€œThe dog barkedâ€ is at coordinate [10, 50, 3].  \n",
    "- The dot for â€œThe puppy made a noiseâ€ is at [11, 51, 4]. (Very close!)  \n",
    "- The dot for â€œI like pizzaâ€ is at [90, 2, 88]. (Far away!)\n",
    "\n",
    "The Translator (Embedding Model) is the tool that takes your text and gives you those coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dec922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The cat watched carefully as the playful kitten chased a leaf across the driveway, only stopping when a car rolled slowly past the house.\n",
      "Vector Length: 768\n",
      "First 5 numbers: [1.0189101696014404, 0.19626553356647491, -2.8098843097686768, 0.09355314821004868, 1.6594434976577759]\n"
     ]
    }
   ],
   "source": [
    "# 1. The text we want to translate\n",
    "text = \"The cat watched carefully as the playful kitten chased a leaf across the driveway, only stopping when a car rolled slowly past the house.\"\n",
    "\n",
    "\n",
    "# 2. Ask Ollama to create the embedding\n",
    "# We use 'nomic-embed-text' because it's built for this.\n",
    "response = ollama.embeddings(\n",
    "    model='nomic-embed-text',\n",
    "    prompt=text\n",
    ")\n",
    "\n",
    "# 3. Get the vector (The list of numbers)\n",
    "vector = response['embedding']\n",
    "\n",
    "# 4. Inspect the result\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Vector Length: {len(vector)}\") # How many dimensions?\n",
    "print(f\"First 5 numbers: {vector[:5]}\") # Just a peek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d6eb7",
   "metadata": {},
   "source": [
    "## Vector Stores\n",
    "\n",
    "#### 1. Intuition: The Color Gradient\n",
    "\n",
    "Imagine a library where books aren't sorted by author or title, but by **color**.\n",
    "\n",
    "- All the **\"Red\"** books (Action/Adventure) are in one corner.\n",
    "- All the **\"Blue\"** books (Sad/Melancholy) are in another.\n",
    "\n",
    "When you walk in and say *\"I want something Red-ish,\"* the librarian doesn't check every book in the building.  \n",
    "They walk straight to the Red corner and grab the closest matches.\n",
    "\n",
    "A **Vector Store** is that library.  \n",
    "It organizes your data so that *\"math-similar\"* items sit next to each other in memory.\n",
    "\n",
    "#### 2. Visual-in-Words: The Index\n",
    "\n",
    "- **Without Vector Store:**  \n",
    "  You have a pile of 1,000 messy papers on the floor.  \n",
    "  To find *\"Project X,\"* you pick up paper #1, read it, put it down.  \n",
    "  Pick up #2, read itâ€¦ *(slow)*\n",
    "\n",
    "- **With Vector Store:**  \n",
    "  You have a specialized filing cabinet where related topics are magnetized together.  \n",
    "  You magnetize your query (*\"Project X\"*) and throw it at the cabinet.  \n",
    "  It sticks immediately to the right folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169bf471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library built! I currently know 3 facts about the dragon.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Initialize the Client (The Librarian)\n",
    "# This creates a temporary in-memory database. \n",
    "# (Use persistent_client to save to disk, but for now, memory is fine)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# 2. Create a Collection (The Bookshelf)\n",
    "# A 'collection' is where we store related documents.\n",
    "collection = client.create_collection(name=\"dragon_facts\") #here since we didn't add any model, so it will download a default embedding model\n",
    "\n",
    "# 3. Add Documents (The Knowledge)\n",
    "# We give each chunk of text an ID so we can find it later.\n",
    "documents = [\n",
    "    \"My dragon's name is Fluffy.\",\n",
    "    \"Fluffy eats 500kg of spicy tacos every Tuesday.\",\n",
    "    \"Fluffy lives in a volcano behind the Walmart.\"\n",
    "]\n",
    "ids = [\"id1\", \"id2\", \"id3\"]\n",
    "\n",
    "# 4. Store them\n",
    "# NOTICE: Chroma handles the embedding part automatically in the background!\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(\"Library built! I currently know\", collection.count(), \"facts about the dragon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1bd49",
   "metadata": {},
   "source": [
    "## Retrival\n",
    "\n",
    "#### 1. Intuition: The \"Needle in the Haystack\"\n",
    "\n",
    "You have a massive haystack (your database).  \n",
    "You throw a magnet (your query) into it.  \n",
    "The magnet pulls out the 3 needles that are most magnetic (most similar).\n",
    "\n",
    "**Retrieval is simply:** Query Vector vs. Database Vectors.\n",
    "\n",
    "#### 2. Visual-in-Words: The Search\n",
    "\n",
    "1. **User Query:**  \n",
    "   *\"What does the dragon eat?\"*\n",
    "\n",
    "2. **Translation:**  \n",
    "   We convert that question into a vector (numbers).\n",
    "\n",
    "3. **Comparison:**  \n",
    "   The database compares that vector against:\n",
    "   - \"Fluffy's Name\"\n",
    "   - \"Fluffy's Food\"\n",
    "   - \"Fluffy's Home\"\n",
    "\n",
    "4. **Ranking:**\n",
    "   - Match with **\"Fluffy Name\"**: 10% match  \n",
    "   - Match with **\"Fluffy Home\"**: 15% match  \n",
    "   - Match with **\"Fluffy Food\"**: 92% match  \n",
    "\n",
    "5. **Result:**  \n",
    "   It returns the **\"Food\"** document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8eee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Tell me about the creature's diet\n",
      "\n",
      "--- Retrieved Document ---\n",
      "Fluffy eats 500kg of spicy tacos every Tuesday.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup (Same as before)\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"dragon_facts_test\")\n",
    "\n",
    "documents = [\n",
    "    \"My dragon's name is Fluffy.\",\n",
    "    \"Fluffy eats 500kg of spicy tacos every Tuesday.\",\n",
    "    \"Fluffy lives in a volcano behind the Walmart.\"\n",
    "]\n",
    "collection.add(documents=documents, ids=[\"id1\", \"id2\", \"id3\"])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. The Retrieval Step\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "query_text = \"Tell me about the creature's diet\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=1  # How many top matches do you want? Let's ask for the best ONE.\n",
    ")\n",
    "\n",
    "# 3. Inspect the results\n",
    "print(\"Question:\", query_text)\n",
    "print(\"\\n--- Retrieved Document ---\")\n",
    "print(results['documents'][0][0]) # It returns a list of lists, so we grab the first item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b02549",
   "metadata": {},
   "source": [
    "## The Full RAG Pipeline\n",
    "\n",
    "1. Setup: Initialize Ollama (Brain) and Chroma (Library).\n",
    "2. Ingest: Read data, Embed it (using Ollama), Store it.\n",
    "3. Ask: User asks a question.\n",
    "4. Retrieve: Find the best chunk.\n",
    "5. Augment: Create a prompt: \"Context: {chunk}. Question: {question}.\"\n",
    "6. Generate: Get the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad7287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Retrieved Context: 'The dragon Fluffy lives in a volcano behind the Walmart.'\n",
      "\n",
      "ðŸ¤– Generating Answer...\n",
      "--------------------------------------------------\n",
      "The beast lives in a volcano behind the Walmart.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. SETUP: Connect the Brain and the Library\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# We tell Chroma: \"Don't use your default. Use Ollama's 'nomic-embed-text' model.\"\n",
    "# Make sure you ran: `ollama pull nomic-embed-text` in terminal first!\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    url=\"http://localhost:11434/api/embeddings\", # Standard Ollama endpoint\n",
    ")\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\n",
    "    name=\"dragon_rag\", \n",
    "    embedding_function=ollama_ef # <--- KEY CHANGE: Using Ollama for embeddings\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. INGEST: Teach the AI\n",
    "# ---------------------------------------------------------\n",
    "# Pro Tip: Repeating the subject (\"The dragon Fluffy\") helps retrieval!\n",
    "docs = [\n",
    "    \"The dragon's name is Fluffy.\",\n",
    "    \"The dragon Fluffy eats 500kg of spicy tacos every Tuesday.\",\n",
    "    \"The dragon Fluffy lives in a volcano behind the Walmart.\"\n",
    "]\n",
    "ids = [\"id1\", \"id2\", \"id3\"]\n",
    "\n",
    "collection.add(documents=docs, ids=ids)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. ASK & RETRIEVE\n",
    "# ---------------------------------------------------------\n",
    "user_question = \"Where does the beast live?\" # Tricky question (Beast != Dragon)\n",
    "\n",
    "# Look it up!\n",
    "results = collection.query(\n",
    "    query_texts=[user_question],\n",
    "    n_results=1\n",
    ")\n",
    "best_chunk = results['documents'][0][0]\n",
    "\n",
    "print(f\"ðŸ”Ž Retrieved Context: '{best_chunk}'\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. AUGMENT & GENERATE\n",
    "# ---------------------------------------------------------\n",
    "# This is the \"Prompt Engineering\" part.\n",
    "prompt = f\"\"\"\n",
    "You are an expert on mythical creatures.\n",
    "Use the following context to answer the question.\n",
    "If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "Context: {best_chunk}\n",
    "\n",
    "Question: {user_question}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ¤– Generating Answer...\")\n",
    "response = ollama.chat(\n",
    "    model='gemma3:4b', # Or 'llama3' or 'mistral'\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "print(response['message']['content'])\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b690dad",
   "metadata": {},
   "source": [
    "## The Butcher (Chunking)\n",
    "\n",
    "#### 1. Intuition: Eating a Steak  \n",
    "You cannot swallow a whole steak in one go. You will choke.  \n",
    "You must use a knife to cut it into bite-sized pieces.  \n",
    "\n",
    "- The **Steak**: Your long document.  \n",
    "- The **Knife**: The Chunker.  \n",
    "- The **Bite**: The Chunk (Vector).\n",
    "\n",
    "#### 2. Visual-in-Words: The Overlap  \n",
    "Imagine cutting a photograph into strips.  \n",
    "If you cut exactly on the edge of a face, you might lose who it is.  \n",
    "\n",
    "So, we use **Overlapping Chunks**.  \n",
    "\n",
    "Chunk 1: *\"...The dragon lives in a volcano. It is hot...\"*  \n",
    "Chunk 2: *\"...volcano. It is hot and filled with lava...\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8cb9f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text Length: 588\n",
      "Number of Chunks: 5\n",
      "--- Sample Chunk 1 ---\n",
      "'ave, a medical certificate is required for absences longer than 2 days.\n",
      "2. Casual Leave: Employees get 5 days of casual leave. This must be applied fo'\n",
      "\n",
      "ðŸ”Ž Found Policy: 'leave. This must be applied for 24 hours in advance.\n",
      "3. Remote Work: Remote work is allowed on Fridays only. \n",
      "   You must be logged into Slack by 9:00'\n",
      "\n",
      "ðŸ¤– AI Answer: Yes, you can work from home, but only on Fridays and you must be logged into Slack by 9:00 AM. Also, you need to apply for leave 24 hours in advance.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. The \"Long\" Document\n",
    "# ---------------------------------------------------------\n",
    "text_file = \"\"\"\n",
    "COMPANY LEAVE POLICY 2024\n",
    "1. Sick Leave: Employees are entitled to 10 days of sick leave per year. \n",
    "   To claim sick leave, a medical certificate is required for absences longer than 2 days.\n",
    "2. Casual Leave: Employees get 5 days of casual leave. This must be applied for 24 hours in advance.\n",
    "3. Remote Work: Remote work is allowed on Fridays only. \n",
    "   You must be logged into Slack by 9:00 AM.\n",
    "4. Lunch Break: Lunch is from 1:00 PM to 2:00 PM. \n",
    "   Eating at desks is strictly prohibited to keep equipment clean.\n",
    "5. Termination: The company requires 2 weeks of notice before resignation.\n",
    "\"\"\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. The Butcher Function (Chunking)\n",
    "# ---------------------------------------------------------\n",
    "def chunk_text(text, chunk_size=100, overlap=20):\n",
    "    # This is a very simple character-based splitter\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        # Create the chunk\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        # Move the window forward, but step back by 'overlap' amount\n",
    "        start += (chunk_size - overlap) \n",
    "    return chunks\n",
    "\n",
    "# Chop the meat!\n",
    "my_chunks = chunk_text(text_file, chunk_size=150, overlap=30)\n",
    "\n",
    "print(f\"Original Text Length: {len(text_file)}\")\n",
    "print(f\"Number of Chunks: {len(my_chunks)}\")\n",
    "print(\"--- Sample Chunk 1 ---\")\n",
    "print(f\"'{my_chunks[1]}'\") # Printing the second chunk to see it\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Setup Database (The same as Lesson 5)\n",
    "# ---------------------------------------------------------\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    url=\"http://localhost:11434/api/embeddings\"\n",
    ")\n",
    "\n",
    "client = chromadb.Client()\n",
    "# Important: We name it differently so we start fresh\n",
    "collection = client.create_collection(name=\"policy_rag\", embedding_function=ollama_ef)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Ingest Chunks\n",
    "# ---------------------------------------------------------\n",
    "# Create IDs automatically: \"id_0\", \"id_1\", \"id_2\"...\n",
    "ids = [f\"id_{i}\" for i in range(len(my_chunks))]\n",
    "\n",
    "collection.add(documents=my_chunks, ids=ids)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Ask a Question\n",
    "# ---------------------------------------------------------\n",
    "question = \"Can I work from home?\"\n",
    "\n",
    "results = collection.query(query_texts=[question], n_results=1)\n",
    "best_chunk = results['documents'][0][0]\n",
    "\n",
    "print(f\"\\nðŸ”Ž Found Policy: '{best_chunk}'\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. Generate Answer\n",
    "# ---------------------------------------------------------\n",
    "prompt = f\"Context: {best_chunk}\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "response = ollama.chat(model='gemma3:4b', messages=[{'role': 'user', 'content': prompt}])\n",
    "print(\"\\nðŸ¤– AI Answer:\", response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a4e5c",
   "metadata": {},
   "source": [
    "## The Elephant's Brain (Chat History)  \n",
    "#### 1. Intuition: The Amnesia Patient & The Diary  \n",
    "The Problem:  \n",
    "If you ask: *\"Who is Fluffy?\"* â†’ It answers: *\"A dragon.\"*  \n",
    "Then you ask: *\"What does he eat?\"* â†’ It asks: *\"Who is 'he'?\"*  \n",
    "\n",
    "The Solution (Memory):  \n",
    "Since the AI has amnesia, we must keep a **Diary** (List) of everything said so far.  \n",
    "Every time we send a new message, we hand the entire Diary to the AI so it can catch up on the context before answering.\n",
    "\n",
    "#### 2. Visual-in-Words: The Stack of Plates  \n",
    "Turn 1: You put a plate down (User: *\"Hi\"*).  \n",
    "The AI puts a plate on top (AI: *\"Hello\"*).  \n",
    "\n",
    "Turn 2: You put a new plate (User: *\"How are you?\"*).  \n",
    "\n",
    "The **Context Window**:  \n",
    "The AI looks at the entire stack of plates from bottom to top to understand the conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c82e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Chat initialized! (Type 'exit' to stop)\n",
      "AI: Fluffy.\n",
      "AI: 500kg of spicy tacos.\n",
      "AI: In a volcano.\n",
      "AI: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. SETUP (Same as before)\n",
    "# ---------------------------------------------------------\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    url=\"http://localhost:11434/api/embeddings\"\n",
    ")\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"dragon_chat_history2\", embedding_function=ollama_ef)\n",
    "\n",
    "# Add Data\n",
    "docs = [\n",
    "    \"The dragon Fluffy eats 500kg of spicy tacos.\",\n",
    "    \"Fluffy lives in a volcano.\"\n",
    "]\n",
    "ids = [\"id1\", \"id2\"]\n",
    "collection.add(documents=docs, ids=ids)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. INITIALIZE MEMORY\n",
    "# ---------------------------------------------------------\n",
    "# We start with a System message to set the behavior\n",
    "conversation_history = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant. Be brief.'}\n",
    "]\n",
    "\n",
    "print(\"ðŸ’¬ Chat initialized! (Type 'exit' to stop)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. THE CHAT LOOP\n",
    "# ---------------------------------------------------------\n",
    "while True:\n",
    "    # A. Get User Input\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # B. Retrieval (RAG)\n",
    "    # We look for facts related to the CURRENT question\n",
    "    results = collection.query(query_texts=[user_input], n_results=1)\n",
    "    context_data = results['documents'][0][0]\n",
    "\n",
    "    # C. Construct the Prompt with Context\n",
    "    # We don't save the HUGE context to history (it wastes space), \n",
    "    # we just use it for this specific turn.\n",
    "    prompt_with_context = f\"\"\"\n",
    "    \n",
    "    You are a strict assistant. You must ONLY use the provided Context info to answer.\n",
    "    If the answer is not in the Context info, you must say \"I don't know\".\n",
    "    Do not use your own external knowledge.\n",
    "\n",
    "    Context info: {context_data}\n",
    "    User Question: {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    # D. Prepare Message List for the AI\n",
    "    # Take the FULL history + the NEW augmented prompt\n",
    "    messages_to_send = conversation_history + [{'role': 'user', 'content': prompt_with_context}]\n",
    "\n",
    "    # E. Generate\n",
    "    response = ollama.chat(\n",
    "        model='gemma3:4b',\n",
    "        messages=messages_to_send\n",
    "    )\n",
    "    ai_answer = response['message']['content']\n",
    "    print(f\"AI: {ai_answer}\")\n",
    "\n",
    "    # F. Update Memory (CRITICAL STEP)\n",
    "    # We save the Original User Question (without the messy context) and the AI Answer\n",
    "    # This keeps the history clean for the next turn.\n",
    "    conversation_history.append({'role': 'user', 'content': user_input})\n",
    "    conversation_history.append({'role': 'assistant', 'content': ai_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd2f130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assistant. Be brief.'},\n",
       " {'role': 'user', 'content': 'Who is the dragon?'},\n",
       " {'role': 'assistant', 'content': 'Fluffy.'},\n",
       " {'role': 'user', 'content': 'What does he eat?'},\n",
       " {'role': 'assistant', 'content': '500kg of spicy tacos.'},\n",
       " {'role': 'user', 'content': 'Where does he live?'},\n",
       " {'role': 'assistant', 'content': 'In a volcano.'},\n",
       " {'role': 'user', 'content': 'Where is the volcano?'},\n",
       " {'role': 'assistant', 'content': \"I don't know.\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe1a4d",
   "metadata": {},
   "source": [
    "## The Judge (Evaluation)\n",
    "\n",
    "You have built the car. You have driven it. You have seen it crash (hallucinate).  \n",
    "Now, how do we measure how good it is without manually chatting for hours?\n",
    "\n",
    "#### 1. Intuition: The Exam  \n",
    "- **Manual Testing:** You ask 100 questions. You read 100 answers. (Takes all day).  \n",
    "- **Automated Evaluation:** You have an â€œAnswer Keyâ€. You write a script to check if the AIâ€™s answer matches the key.\n",
    "\n",
    "#### 2. Visual-in-Words: RAGAS (The Scorecard)  \n",
    "There is a standard framework called **RAGAS** (Retrieval-Augmented Generation Assessment). It measures:  \n",
    "\n",
    "- **Faithfulness:** Did the AI stick to the context? (No â€œMystariaâ€.)  \n",
    "- **Answer Relevance:** Did it actually answer the question? (No â€œI like turtles.â€)  \n",
    "- **Context Precision:** Did the retrieval find the right chunk?  \n",
    "\n",
    "Since installing RAGAS libraries can be heavy, we will build a **Simple Judge** using the LLM itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9831f6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 Score (Expected 1): 1\n",
      "Test 2 Score (Expected 0): 0\n",
      "Test 3 score (Expected 0): 1\n",
      "\n",
      "The answer \"4\" is a simple arithmetic calculation that is not related to the context about dragons eating tacos. It accurately represents the sum of 2 + 2, making it faithful to the math context implied by the question itself, but not relevant or derived from the provided context. Therefore, I score it 1 for being an accurate answer in isolation.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# The Judge Function\n",
    "# ---------------------------------------------------------\n",
    "def evaluate_rag(question, context, answer):\n",
    "    # The Rubric for the Judge\n",
    "    judge_prompt = f\"\"\"\n",
    "    You are a grader. \n",
    "    1. Read the QUESTION.\n",
    "    2. Read the CONTEXT (The facts).\n",
    "    3. Read the ANSWER (What the AI said).\n",
    "    \n",
    "    Task: Determine if the ANSWER is fully supported by the CONTEXT.\n",
    "    If the answer contains information NOT in the context (hallucination), score it 0.\n",
    "    If the answer is faithful to the context, score it 1.\n",
    "    \n",
    "    Return ONLY the number: 0 or 1.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    CONTEXT: {context}\n",
    "    ANSWER: {answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='llama3.1:8b-instruct-q4_K_M', # The Judge needs to be smart!\n",
    "        messages=[{'role': 'user', 'content': judge_prompt}]\n",
    "    )\n",
    "    \n",
    "    score = response['message']['content'].strip()\n",
    "    return score\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test Case 1: The Good Answer\n",
    "# ---------------------------------------------------------\n",
    "q1 = \"Where does he live?\"\n",
    "c1 = \"Fluffy lives in a volcano behind Walmart.\"\n",
    "a1 = \"He lives in a volcano behind Walmart.\" # Good answer\n",
    "\n",
    "score1 = evaluate_rag(q1, c1, a1)\n",
    "print(f\"Test 1 Score (Expected 1): {score1}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test Case 2: The Hallucination (Mystaria)\n",
    "# ---------------------------------------------------------\n",
    "q2 = \"Where is the volcano?\"\n",
    "c2 = \"Fluffy lives in a volcano behind Walmart.\"\n",
    "a2 = \"The volcano is on the island of Mystaria.\" # BAD answer\n",
    "\n",
    "score2 = evaluate_rag(q2, c2, a2)\n",
    "print(f\"Test 2 Score (Expected 0): {score2}\")\n",
    "\n",
    "q3 = \"What is 2+2?\"\n",
    "c3 = \"The dragon eats tacos.\"\n",
    "a3 = \"4\"\n",
    "\n",
    "score3 = evaluate_rag(q3,c3,a3)\n",
    "print(f\"Test 3 score (Expected 0): {score3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ae403",
   "metadata": {},
   "source": [
    "Score3 is a **FAILURE** of the Judge!  \n",
    "**Why?**\n",
    "\n",
    "Context: *\"The dragon eats tacos.\"*  \n",
    "Answer: *\"4\"*.  \n",
    "\n",
    "Does the sentence *\"The dragon eats tacos\"* support the fact that 2+2=4?  \n",
    "**NO.**  \n",
    "Therefore, the score should be **0**.\n",
    "\n",
    "The Judge (Llama 3.2) got confused because it *knows* 2+2=4 is true in the real world, so it hesitated to mark it â€œFalseâ€.  \n",
    "It didnâ€™t strictly follow the rule **â€œSupported by Contextâ€**.\n",
    "\n",
    "How to make the Judge stricter?  \n",
    "You need to explicitly tell the Judge to **ignore its own brain**.\n",
    "\n",
    "Change the prompt to:\n",
    "\n",
    "```python\n",
    "Task: Determine if the ANSWER can be derived EXCLUSIVELY from the CONTEXT.\n",
    "    Ignore your own knowledge. \n",
    "    If the context does not contain the answer, score it 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b36dea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 Score (Expected 1): 1\n",
      "Test 2 Score (Expected 0): 0\n",
      "Test 3 score (Expected 0): 0\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# The Judge Function\n",
    "# ---------------------------------------------------------\n",
    "def evaluate_rag(question, context, answer):\n",
    "    # The Rubric for the Judge\n",
    "    judge_prompt = f\"\"\"\n",
    "    You are a grader. \n",
    "    1. Read the QUESTION.\n",
    "    2. Read the CONTEXT (The facts).\n",
    "    3. Read the ANSWER (What the AI said).\n",
    "    \n",
    "    Task: Determine if the ANSWER can be derived EXCLUSIVELY from the CONTEXT.\n",
    "    Ignore your own knowledge. \n",
    "    If the context does not contain the answer, score it 0. Else 1.\n",
    "    \n",
    "    Return ONLY the number: 0 or 1.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    CONTEXT: {context}\n",
    "    ANSWER: {answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model='llama3.1:8b-instruct-q4_K_M', # The Judge needs to be smart!\n",
    "        messages=[{'role': 'user', 'content': judge_prompt}]\n",
    "    )\n",
    "    \n",
    "    score = response['message']['content'].strip()\n",
    "    return score\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test Case 1: The Good Answer\n",
    "# ---------------------------------------------------------\n",
    "q1 = \"Where does he live?\"\n",
    "c1 = \"Fluffy lives in a volcano behind Walmart.\"\n",
    "a1 = \"He lives in a volcano behind Walmart.\" # Good answer\n",
    "\n",
    "score1 = evaluate_rag(q1, c1, a1)\n",
    "print(f\"Test 1 Score (Expected 1): {score1}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Test Case 2: The Hallucination (Mystaria)\n",
    "# ---------------------------------------------------------\n",
    "q2 = \"Where is the volcano?\"\n",
    "c2 = \"Fluffy lives in a volcano behind Walmart.\"\n",
    "a2 = \"The volcano is on the island of Mystaria.\" # BAD answer\n",
    "\n",
    "score2 = evaluate_rag(q2, c2, a2)\n",
    "print(f\"Test 2 Score (Expected 0): {score2}\")\n",
    "\n",
    "q3 = \"What is 2+2?\"\n",
    "c3 = \"The dragon eats tacos.\"\n",
    "a3 = \"4\"\n",
    "\n",
    "score3 = evaluate_rag(q3,c3,a3)\n",
    "print(f\"Test 3 score (Expected 0): {score3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e57d6f",
   "metadata": {},
   "source": [
    "## Final Graduation Project\n",
    "Objective: Build a â€œPersonal Journal Chatbotâ€.\n",
    "\n",
    "Data:  \n",
    "Create a text file `journal.txt`. Write fake entries about your life.  \n",
    "Example lines:  \n",
    "- Jan 1: I started learning Guitar.  \n",
    "- Jan 5: My guitar string broke.  \n",
    "- Feb 10: I bought a new blue guitar.\n",
    "\n",
    "Pipeline:\n",
    "1. Load and chunk the file.  \n",
    "2. Store in ChromaDB (using Ollama embeddings).  \n",
    "3. Create a loop (`while True:`).  \n",
    "4. Add memory (history).  \n",
    "5. Use the strict prompt: *â€œI donâ€™t know if not in context.â€*  \n",
    "\n",
    "Test:\n",
    "- Ask: *â€œWhat instrument do I play?â€*  \n",
    "- Ask: *â€œWhat happened in January?â€*  \n",
    "- Ask: *â€œWhat color is my car?â€* (Should reply *â€œI donâ€™t know.â€*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60683ad",
   "metadata": {},
   "source": [
    "#### Better Test Cases\n",
    "1. The â€œSpecific Detailâ€ Test  \n",
    "Q: *â€œWhat did I buy in February?â€*  \n",
    "Expected: Mention the **blue guitar** (Feb 10).  \n",
    "Must **not** mention the capo (Jan 22).\n",
    "\n",
    "2. The â€œSynthesisâ€ Test (Combining facts)  \n",
    "Q: *â€œDid I ever perform in public?â€*  \n",
    "Expected: Find **Mar 9 (Open Mic)** or **Jul 20 (Coffee Shop)** and reply **â€œYesâ€**.\n",
    "\n",
    "3. The â€œInferenceâ€ Test (Logic)  \n",
    "Q: *â€œDo I know how to read music?â€*  \n",
    "Expected: Spot **Oct 2 (â€œI learned to read simple tablatureâ€)** and say **â€œYesâ€** or **â€œYou can read tablatureâ€**.\n",
    "\n",
    "4. The â€œTrickâ€ Question (The Pivot)  \n",
    "Q: *â€œDo I play the piano?â€*  \n",
    "Expected: **â€œI donâ€™t knowâ€** (or **â€œNoâ€**).  \n",
    "Reasoning: **May 21** says you *collaborated with a pianist*â€”you are **not** the pianist.\n",
    "\n",
    "5. The â€œGoalâ€ Question  \n",
    "Q: *â€œWhat do you want to do next year?â€*  \n",
    "Expected: Find **Dec 5** and mention **â€œMusic Theoryâ€**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494d9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat initialized! (Type 'exit' to stop)\n",
      "\n",
      "--- ðŸ”Ž DEBUG: Retrieved 5 chunks ---\n",
      "My Journal\n",
      "\n",
      "Jan 1: I started learning Guitar.\n",
      "\n",
      "Jan 8: I practiced chords for an hour and finally switched between G and C without fumbling.\n",
      "\n",
      "Jan 22: I bought a set of extra guitar strings and a capo.\n",
      "\n",
      "Feb 10: I bought a new blue guitar.\n",
      "--------------------------------------------------\n",
      "\n",
      "AI: A new blue guitar.\n",
      "\n",
      "--- ðŸ”Ž DEBUG: Retrieved 5 chunks ---\n",
      "My Journal\n",
      "\n",
      "Mar 9: I discovered a local open mic and signed up to play a 2-minute song.\n",
      "\n",
      "Jul 20: I played a short set at the neighborhood coffee shop; it felt exhilarating.\n",
      "\n",
      "Jan 18: I met a neighbor who used to play in a college band and gave me tips.\n",
      "\n",
      "May 21: I collaborated with a pianist online for a cover song.\n",
      "--------------------------------------------------\n",
      "\n",
      "AI: Yes.\n",
      "\n",
      "--- ðŸ”Ž DEBUG: Retrieved 5 chunks ---\n",
      "Oct 2: I learned to read simple tablature and it unlocked lots of new songs.\n",
      "\n",
      "Mar 3: I learned about tuning by ear and practiced matching notes.\n",
      "\n",
      "Feb 1: I learned my first full song â€” a simple three-chord tune.\n",
      "\n",
      "My Journal\n",
      "\n",
      "Apr 12: I strummed through a song without looking at my fingers for the first time.\n",
      "--------------------------------------------------\n",
      "\n",
      "AI: No.\n",
      "\n",
      "--- ðŸ”Ž DEBUG: Retrieved 5 chunks ---\n",
      "May 21: I collaborated with a pianist online for a cover song.\n",
      "\n",
      "My Journal\n",
      "\n",
      "Jan 8: I practiced chords for an hour and finally switched between G and C without fumbling.\n",
      "\n",
      "Jan 12: I watched a tutorial on fingerpicking and tried it on an old melody.\n",
      "\n",
      "Oct 2: I learned to read simple tablature and it unlocked lots of new songs.\n",
      "--------------------------------------------------\n",
      "\n",
      "AI: No.\n",
      "\n",
      "--- ðŸ”Ž DEBUG: Retrieved 5 chunks ---\n",
      "Dec 5: I reflected on the year and set a goal to learn music theory next year.\n",
      "\n",
      "My Journal\n",
      "\n",
      "Jan 1: I started learning Guitar.\n",
      "\n",
      "Oct 2: I learned to read simple tablature and it unlocked lots of new songs.\n",
      "\n",
      "Jan 8: I practiced chords for an hour and finally switched between G and C without fumbling.\n",
      "--------------------------------------------------\n",
      "\n",
      "AI: I want to learn music theory.\n"
     ]
    }
   ],
   "source": [
    "with open (\"journal.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_file = file.read()\n",
    "\n",
    "def text_chunk(text, chunk_size=None, overlap=None):\n",
    "    # 1. We ignore chunk_size/overlap because we are trusting the document structure.\n",
    "    # 2. Split the text wherever there is a double newline.\n",
    "    chunks = text.split('\\n\\n')\n",
    "    \n",
    "    # 3. Clean up: Remove any empty items or extra whitespace\n",
    "    clean_chunks = [c.strip() for c in chunks if c.strip()]\n",
    "    \n",
    "    return clean_chunks\n",
    "\n",
    "my_chunks = text_chunk(text_file, chunk_size=250, overlap= 75)\n",
    "\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(model_name= \"nomic-embed-text\", url=\"http://localhost:11434/api/embeddings\")\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"my_collection\", embedding_function= ollama_ef)\n",
    "\n",
    "ids = [f\"id_{i}\" for i in range(len(my_chunks))]\n",
    "\n",
    "collection.add(documents= my_chunks, ids=ids)\n",
    "\n",
    "convo_history = [\n",
    "    {'role':'system', 'content':'You are a helpful assitant. Be brief.'}\n",
    "]\n",
    "print(\"Chat initialized! (Type 'exit' to stop)\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    user_input = input(\"/nYou\")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    result = collection.query(query_texts=[user_input], n_results=5)\n",
    "    list_of_chunks = result['documents'][0]\n",
    "    context_data = \"\\n\\n\".join(list_of_chunks)\n",
    "\n",
    "    print(f\"\\n--- ðŸ”Ž DEBUG: Retrieved {len(list_of_chunks)} chunks ---\")\n",
    "    print(context_data)\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    prompt_with_context = f\"\"\"\n",
    "    You are a strict assistant. You must ONLY use the provided Context info to answer.\n",
    "    If the answer is not in the Context info, you must say \"I don't know\".\n",
    "    Do not use your own external knowledge.\n",
    "\n",
    "    Context info: {context_data}\n",
    "    User Question: {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    messages_to_send = convo_history + [{'role': 'user', 'content': prompt_with_context}]    \n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.1:8b-instruct-q4_K_M\",\n",
    "        messages=messages_to_send\n",
    "    )\n",
    "    ai_answer = response['message']['content']\n",
    "    print(f\"AI: {ai_answer}\")\n",
    "\n",
    "    convo_history.append({'role': 'user', 'content': user_input})\n",
    "    convo_history.append({'role': 'assistant', 'content': ai_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a55cd6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'You are a helpful assitant. Be brief.'},\n",
       " {'role': 'user', 'content': 'What did I buy in February?'},\n",
       " {'role': 'assistant', 'content': 'A new blue guitar.'},\n",
       " {'role': 'user', 'content': 'Did I ever perform in public?'},\n",
       " {'role': 'assistant', 'content': 'Yes.'},\n",
       " {'role': 'user', 'content': 'Do I know how to read music?'},\n",
       " {'role': 'assistant', 'content': 'No.'},\n",
       " {'role': 'user', 'content': 'Do I play the piano?'},\n",
       " {'role': 'assistant', 'content': 'No.'},\n",
       " {'role': 'user', 'content': 'What do you want to do next year?'},\n",
       " {'role': 'assistant', 'content': 'I want to learn music theory.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0032ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for Universal chunker incase, we dont know the document structure.\n",
    "import re\n",
    "\n",
    "def universal_chunker(text, target_chunk_size=500, overlap_size=100):\n",
    "    # 1. Clean the text (remove excessive newlines)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 2. Split into sentences using Regex (looks for punctuation followed by space)\n",
    "    # This splits by \".\", \"!\", or \"?\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_len = len(sentence)\n",
    "        \n",
    "        # Check if adding this sentence would exceed the limit\n",
    "        if current_length + sentence_len > target_chunk_size:\n",
    "            # A. Save the current chunk\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            \n",
    "            # B. Handle Overlap (The Safety Net)\n",
    "            # We keep the last few sentences so the context flows into the next chunk\n",
    "            overlap_buffer = []\n",
    "            overlap_count = 0\n",
    "            \n",
    "            # Go backwards through the current chunk to grab sentences for overlap\n",
    "            for s in reversed(current_chunk):\n",
    "                if overlap_count < overlap_size:\n",
    "                    overlap_buffer.insert(0, s) # Add to front\n",
    "                    overlap_count += len(s)\n",
    "                else:\n",
    "                    break # Stop when we have enough overlap\n",
    "            \n",
    "            # Start the new chunk with the overlap sentences\n",
    "            current_chunk = overlap_buffer\n",
    "            current_length = overlap_count\n",
    "            \n",
    "        # Add the new sentence\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_len\n",
    "        \n",
    "    # Add the final leftover bits\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "    return chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
