{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26091c2",
   "metadata": {},
   "source": [
    "## Understanding how to generate reponses using Ollama LLMs\n",
    "\n",
    "#### 1. Intuition: The \"Open-Book Exam\"  \n",
    "Imagine you are a brilliant student (the LLM), but you have amnesia.  \n",
    "You remember everything up to 2023, but nothing after that.  \n",
    "\n",
    "- **Without RAG:**  \n",
    "  If I ask you about news from today, you might panic and make something up to sound smart.  \n",
    "  This is called **Hallucination**.  \n",
    "\n",
    "- **With RAG:**  \n",
    "  Before you answer, I hand you a newspaper from today.  \n",
    "  You read the specific article relevant to my question, and then you answer using that information.  \n",
    "\n",
    "**RAG is simply the process of handing the LLM the right notes before it speaks.**\n",
    "\n",
    "#### 2. Visual-in-Words: The Architecture  \n",
    "Close your eyes and visualize this flow:  \n",
    "\n",
    "1. The **User** asks a question:  \n",
    "   *\"How do I reset the X-2000 router?\"*  \n",
    "\n",
    "2. The **Retriever** (The Hand) reaches into a bucket of manuals (your database), rummages around, and grabs the one specific paragraph about resetting the X-2000.  \n",
    "\n",
    "3. The **Augmenter** (The Glue) takes the User's question and tapes that paragraph to it.  \n",
    "   New Prompt:  \n",
    "   *\"Context: [Reset paragraph]. Question: How do I reset the X-2000 router?\"*  \n",
    "\n",
    "4. The **Generator** (The Brain / ollama) reads the combined text and generates an accurate answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0b202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Answer: The capital of India is **New Delhi**. \n",
      "\n",
      "Itâ€™s a planned city, officially named New Delhi, though the larger area is officially called National Capital Territory of Delhi. ðŸ˜Š \n",
      "\n",
      "Do you want to know anything more about New Delhi?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# 1. Define the question\n",
    "user_question = \"What is the capital of India?\"\n",
    "\n",
    "# 2. Send it to the model (Generation only)\n",
    "# We are NOT providing external context yet.\n",
    "response = ollama.chat(\n",
    "    model='gemma3:4b',  # Enter the model that you pull on ollama\n",
    "    messages=[{'role': 'user', 'content': user_question}]\n",
    ")\n",
    "\n",
    "# 3. Print the answer\n",
    "print(\"AI Answer:\", response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0e749",
   "metadata": {},
   "source": [
    "We write,<br>\n",
    "`response['message']['content']`<br>\n",
    "because it is in JSON format:<br>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model\": \"phi3:3.8b\",\n",
    "  \"created_at\": \"2025-12-15T09:45:00.5105742Z\",\n",
    "  \"done\": true,\n",
    "  \"done_reason\": \"stop\",\n",
    "  \"total_duration\": 11129766800,\n",
    "  \"load_duration\": 3728855400,\n",
    "  \"prompt_eval_count\": 16,\n",
    "  \"prompt_eval_duration\": 765201100,\n",
    "  \"eval_count\": 94,\n",
    "  \"eval_duration\": 6592721000,\n",
    "  \"message\": {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"The capital of India is New Delhi. It became the political center in 1956 when the country's government moved from Kolkata (then called Calcutta) and now serves as a major hub for culture, education, and politics, housing Parliament House, which holds the Indian legislative bodies - Lok Sabha (House of the People), Rajya Sabha (Council of States), along with numerous other government offices.\",\n",
    "    \"thinking\": null,\n",
    "    \"images\": null,\n",
    "    \"tool_name\": null,\n",
    "    \"tool_calls\": null\n",
    "  },\n",
    "  \"logprobs\": null\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8660b",
   "metadata": {},
   "source": [
    "### Different Roles\n",
    "\n",
    "| Role      | Analogy               | Purpose                                  |\n",
    "|-----------|-----------------------|------------------------------------------|\n",
    "| System    | The Boss / Director   | Defines behavior, tone, and rules.        |\n",
    "| User      | The Customer          | Asks the question.                       |\n",
    "| Assistant | The Transcript        | Provides conversation history/context.   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a01c85",
   "metadata": {},
   "source": [
    "```py\n",
    "messages=[\n",
    "    # 1. We set the context (optional but good)\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "\n",
    "    # 2. First turn\n",
    "    {'role': 'user', 'content': 'Who wrote Harry Potter?'},\n",
    "    {'role': 'assistant', 'content': 'J.K. Rowling wrote it.'}, # <--- WE INSERT THIS MANUALLY OR FROM PREVIOUS RESPONSE\n",
    "\n",
    "    # 3. Current question (The AI now reads the line above and knows 'she' = Rowling)\n",
    "    {'role': 'user', 'content': 'What year was she born?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3bcef",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "#### 1. Intuition: The Supermarket Aisle  \n",
    "Imagine you go to a supermarket and ask a clerk: â€œI need ingredients for a salad.â€  \n",
    "\n",
    "- **Keyword Search (The old Way):**  \n",
    "The clerk looks for items that have the word â€œSaladâ€ written on them.  \n",
    "They hand you â€œSalad Dressingâ€ and â€œSalad Tongsâ€.  \n",
    "They miss the Lettuce and Tomatoes because those items donâ€™t have the word â€œSaladâ€ printed on them.  \n",
    "\n",
    "- **Vector Search (Embeddings â€“ The RAG Way):**  \n",
    "The clerk understands the concept of a salad.  \n",
    "They know that Lettuce, Tomatoes, and Cucumbers â€œliveâ€ in the same conceptual aisle as â€œSaladâ€, even if the words are different.  \n",
    "\n",
    "Embeddings turn text into a coordinate on a map.  \n",
    "Words with similar meanings live close together.\n",
    "\n",
    "#### 2. Visual-in-Words: The 3-D Map  \n",
    "Imagine a giant 3-D box floating in space.  \n",
    "Every sentence you can possibly speak is a tiny dot inside this box.  \n",
    "\n",
    "- The dot for â€œThe dog barkedâ€ is at coordinate [10, 50, 3].  \n",
    "- The dot for â€œThe puppy made a noiseâ€ is at [11, 51, 4]. (Very close!)  \n",
    "- The dot for â€œI like pizzaâ€ is at [90, 2, 88]. (Far away!)\n",
    "\n",
    "The Translator (Embedding Model) is the tool that takes your text and gives you those coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dec922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The cat watched carefully as the playful kitten chased a leaf across the driveway, only stopping when a car rolled slowly past the house.\n",
      "Vector Length: 768\n",
      "First 5 numbers: [1.0189101696014404, 0.19626553356647491, -2.8098843097686768, 0.09355314821004868, 1.6594434976577759]\n"
     ]
    }
   ],
   "source": [
    "# 1. The text we want to translate\n",
    "text = \"The cat watched carefully as the playful kitten chased a leaf across the driveway, only stopping when a car rolled slowly past the house.\"\n",
    "\n",
    "\n",
    "# 2. Ask Ollama to create the embedding\n",
    "# We use 'nomic-embed-text' because it's built for this.\n",
    "response = ollama.embeddings(\n",
    "    model='nomic-embed-text',\n",
    "    prompt=text\n",
    ")\n",
    "\n",
    "# 3. Get the vector (The list of numbers)\n",
    "vector = response['embedding']\n",
    "\n",
    "# 4. Inspect the result\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Vector Length: {len(vector)}\") # How many dimensions?\n",
    "print(f\"First 5 numbers: {vector[:5]}\") # Just a peek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d6eb7",
   "metadata": {},
   "source": [
    "## Vector Stores\n",
    "\n",
    "#### 1. Intuition: The Color Gradient\n",
    "\n",
    "Imagine a library where books aren't sorted by author or title, but by **color**.\n",
    "\n",
    "- All the **\"Red\"** books (Action/Adventure) are in one corner.\n",
    "- All the **\"Blue\"** books (Sad/Melancholy) are in another.\n",
    "\n",
    "When you walk in and say *\"I want something Red-ish,\"* the librarian doesn't check every book in the building.  \n",
    "They walk straight to the Red corner and grab the closest matches.\n",
    "\n",
    "A **Vector Store** is that library.  \n",
    "It organizes your data so that *\"math-similar\"* items sit next to each other in memory.\n",
    "\n",
    "#### 2. Visual-in-Words: The Index\n",
    "\n",
    "- **Without Vector Store:**  \n",
    "  You have a pile of 1,000 messy papers on the floor.  \n",
    "  To find *\"Project X,\"* you pick up paper #1, read it, put it down.  \n",
    "  Pick up #2, read itâ€¦ *(slow)*\n",
    "\n",
    "- **With Vector Store:**  \n",
    "  You have a specialized filing cabinet where related topics are magnetized together.  \n",
    "  You magnetize your query (*\"Project X\"*) and throw it at the cabinet.  \n",
    "  It sticks immediately to the right folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169bf471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library built! I currently know 3 facts about the dragon.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# 1. Initialize the Client (The Librarian)\n",
    "# This creates a temporary in-memory database. \n",
    "# (Use persistent_client to save to disk, but for now, memory is fine)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# 2. Create a Collection (The Bookshelf)\n",
    "# A 'collection' is where we store related documents.\n",
    "collection = client.create_collection(name=\"dragon_facts\") #here since we didn't add any model, so it will download a default embedding model\n",
    "\n",
    "# 3. Add Documents (The Knowledge)\n",
    "# We give each chunk of text an ID so we can find it later.\n",
    "documents = [\n",
    "    \"My dragon's name is Fluffy.\",\n",
    "    \"Fluffy eats 500kg of spicy tacos every Tuesday.\",\n",
    "    \"Fluffy lives in a volcano behind the Walmart.\"\n",
    "]\n",
    "ids = [\"id1\", \"id2\", \"id3\"]\n",
    "\n",
    "# 4. Store them\n",
    "# NOTICE: Chroma handles the embedding part automatically in the background!\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(\"Library built! I currently know\", collection.count(), \"facts about the dragon.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1bd49",
   "metadata": {},
   "source": [
    "## Retrival\n",
    "\n",
    "#### 1. Intuition: The \"Needle in the Haystack\"\n",
    "\n",
    "You have a massive haystack (your database).  \n",
    "You throw a magnet (your query) into it.  \n",
    "The magnet pulls out the 3 needles that are most magnetic (most similar).\n",
    "\n",
    "**Retrieval is simply:** Query Vector vs. Database Vectors.\n",
    "\n",
    "#### 2. Visual-in-Words: The Search\n",
    "\n",
    "1. **User Query:**  \n",
    "   *\"What does the dragon eat?\"*\n",
    "\n",
    "2. **Translation:**  \n",
    "   We convert that question into a vector (numbers).\n",
    "\n",
    "3. **Comparison:**  \n",
    "   The database compares that vector against:\n",
    "   - \"Fluffy's Name\"\n",
    "   - \"Fluffy's Food\"\n",
    "   - \"Fluffy's Home\"\n",
    "\n",
    "4. **Ranking:**\n",
    "   - Match with **\"Fluffy Name\"**: 10% match  \n",
    "   - Match with **\"Fluffy Home\"**: 15% match  \n",
    "   - Match with **\"Fluffy Food\"**: 92% match  \n",
    "\n",
    "5. **Result:**  \n",
    "   It returns the **\"Food\"** document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad8eee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Tell me about the creature's diet\n",
      "\n",
      "--- Retrieved Document ---\n",
      "Fluffy eats 500kg of spicy tacos every Tuesday.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup (Same as before)\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"dragon_facts_test\")\n",
    "\n",
    "documents = [\n",
    "    \"My dragon's name is Fluffy.\",\n",
    "    \"Fluffy eats 500kg of spicy tacos every Tuesday.\",\n",
    "    \"Fluffy lives in a volcano behind the Walmart.\"\n",
    "]\n",
    "collection.add(documents=documents, ids=[\"id1\", \"id2\", \"id3\"])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. The Retrieval Step\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "query_text = \"Tell me about the creature's diet\"\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=1  # How many top matches do you want? Let's ask for the best ONE.\n",
    ")\n",
    "\n",
    "# 3. Inspect the results\n",
    "print(\"Question:\", query_text)\n",
    "print(\"\\n--- Retrieved Document ---\")\n",
    "print(results['documents'][0][0]) # It returns a list of lists, so we grab the first item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b02549",
   "metadata": {},
   "source": [
    "## The Full RAG Pipeline\n",
    "\n",
    "1. Setup: Initialize Ollama (Brain) and Chroma (Library).\n",
    "2. Ingest: Read data, Embed it (using Ollama), Store it.\n",
    "3. Ask: User asks a question.\n",
    "4. Retrieve: Find the best chunk.\n",
    "5. Augment: Create a prompt: \"Context: {chunk}. Question: {question}.\"\n",
    "6. Generate: Get the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad7287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Retrieved Context: 'The dragon Fluffy lives in a volcano behind the Walmart.'\n",
      "\n",
      "ðŸ¤– Generating Answer...\n",
      "--------------------------------------------------\n",
      "The beast lives in a volcano behind the Walmart.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP: Connect the Brain and the Library\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# We tell Chroma: \"Don't use your default. Use Ollama's 'nomic-embed-text' model.\"\n",
    "# Make sure you ran: `ollama pull nomic-embed-text` in terminal first!\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    url=\"http://localhost:11434/api/embeddings\", # Standard Ollama endpoint\n",
    ")\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\n",
    "    name=\"dragon_rag\", \n",
    "    embedding_function=ollama_ef # <--- KEY CHANGE: Using Ollama for embeddings\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. INGEST: Teach the AI\n",
    "# ---------------------------------------------------------\n",
    "# Pro Tip: Repeating the subject (\"The dragon Fluffy\") helps retrieval!\n",
    "docs = [\n",
    "    \"The dragon's name is Fluffy.\",\n",
    "    \"The dragon Fluffy eats 500kg of spicy tacos every Tuesday.\",\n",
    "    \"The dragon Fluffy lives in a volcano behind the Walmart.\"\n",
    "]\n",
    "ids = [\"id1\", \"id2\", \"id3\"]\n",
    "\n",
    "collection.add(documents=docs, ids=ids)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. ASK & RETRIEVE\n",
    "# ---------------------------------------------------------\n",
    "user_question = \"Where does the beast live?\" # Tricky question (Beast != Dragon)\n",
    "\n",
    "# Look it up!\n",
    "results = collection.query(\n",
    "    query_texts=[user_question],\n",
    "    n_results=1\n",
    ")\n",
    "best_chunk = results['documents'][0][0]\n",
    "\n",
    "print(f\"ðŸ”Ž Retrieved Context: '{best_chunk}'\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. AUGMENT & GENERATE\n",
    "# ---------------------------------------------------------\n",
    "# This is the \"Prompt Engineering\" part.\n",
    "prompt = f\"\"\"\n",
    "You are an expert on mythical creatures.\n",
    "Use the following context to answer the question.\n",
    "If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "Context: {best_chunk}\n",
    "\n",
    "Question: {user_question}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ¤– Generating Answer...\")\n",
    "response = ollama.chat(\n",
    "    model='gemma3:4b', # Or 'llama3' or 'mistral'\n",
    "    messages=[{'role': 'user', 'content': prompt}]\n",
    ")\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "print(response['message']['content'])\n",
    "print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a4e5c",
   "metadata": {},
   "source": [
    "## The Elephant's Brain (Chat History)  \n",
    "#### 1. Intuition: The Amnesia Patient & The Diary  \n",
    "The Problem:  \n",
    "If you ask: *\"Who is Fluffy?\"* â†’ It answers: *\"A dragon.\"*  \n",
    "Then you ask: *\"What does he eat?\"* â†’ It asks: *\"Who is 'he'?\"*  \n",
    "\n",
    "The Solution (Memory):  \n",
    "Since the AI has amnesia, we must keep a **Diary** (List) of everything said so far.  \n",
    "Every time we send a new message, we hand the entire Diary to the AI so it can catch up on the context before answering.\n",
    "\n",
    "#### 2. Visual-in-Words: The Stack of Plates  \n",
    "Turn 1: You put a plate down (User: *\"Hi\"*).  \n",
    "The AI puts a plate on top (AI: *\"Hello\"*).  \n",
    "\n",
    "Turn 2: You put a new plate (User: *\"How are you?\"*).  \n",
    "\n",
    "The **Context Window**:  \n",
    "The AI looks at the entire stack of plates from bottom to top to understand the conversation flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c82e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Chat initialized! (Type 'exit' to stop)\n",
      "AI: Fluffy is the dragon.\n",
      "AI: 500kg of spicy tacos.\n",
      "AI: I don't know.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. SETUP (Same as before)\n",
    "# ---------------------------------------------------------\n",
    "ollama_ef = embedding_functions.OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    url=\"http://localhost:11434/api/embeddings\"\n",
    ")\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"dragon_chat_history2\", embedding_function=ollama_ef)\n",
    "\n",
    "# Add Data\n",
    "docs = [\n",
    "    \"The dragon Fluffy eats 500kg of spicy tacos.\",\n",
    "    \"Fluffy lives in a volcano.\"\n",
    "]\n",
    "ids = [\"id1\", \"id2\"]\n",
    "collection.add(documents=docs, ids=ids)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. INITIALIZE MEMORY\n",
    "# ---------------------------------------------------------\n",
    "# We start with a System message to set the behavior\n",
    "conversation_history = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant. Be brief.'}\n",
    "]\n",
    "\n",
    "print(\"ðŸ’¬ Chat initialized! (Type 'exit' to stop)\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. THE CHAT LOOP\n",
    "# ---------------------------------------------------------\n",
    "while True:\n",
    "    # A. Get User Input\n",
    "    user_input = input(\"\\nYou: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # B. Retrieval (RAG)\n",
    "    # We look for facts related to the CURRENT question\n",
    "    results = collection.query(query_texts=[user_input], n_results=1)\n",
    "    context_data = results['documents'][0][0]\n",
    "\n",
    "    # C. Construct the Prompt with Context\n",
    "    # We don't save the HUGE context to history (it wastes space), \n",
    "    # we just use it for this specific turn.\n",
    "    prompt_with_context = f\"\"\"\n",
    "    \n",
    "    You are a strict assistant. You must ONLY use the provided Context info to answer.\n",
    "    If the answer is not in the Context info, you must say \"I don't know\".\n",
    "    Do not use your own external knowledge.\n",
    "\n",
    "    Context info: {context_data}\n",
    "    User Question: {user_input}\n",
    "    \"\"\"\n",
    "\n",
    "    # D. Prepare Message List for the AI\n",
    "    # Take the FULL history + the NEW augmented prompt\n",
    "    messages_to_send = conversation_history + [{'role': 'user', 'content': prompt_with_context}]\n",
    "\n",
    "    # E. Generate\n",
    "    response = ollama.chat(\n",
    "        model='gemma3:4b',\n",
    "        messages=messages_to_send\n",
    "    )\n",
    "    ai_answer = response['message']['content']\n",
    "    print(f\"AI: {ai_answer}\")\n",
    "\n",
    "    # F. Update Memory (CRITICAL STEP)\n",
    "    # We save the Original User Question (without the messy context) and the AI Answer\n",
    "    # This keeps the history clean for the next turn.\n",
    "    conversation_history.append({'role': 'user', 'content': user_input})\n",
    "    conversation_history.append({'role': 'assistant', 'content': ai_answer})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe1a4d",
   "metadata": {},
   "source": [
    "## The Judge (Evaluation)\n",
    "\n",
    "You have built the car. You have driven it. You have seen it crash (hallucinate).  \n",
    "Now, how do we measure how good it is without manually chatting for hours?\n",
    "\n",
    "#### 1. Intuition: The Exam  \n",
    "- **Manual Testing:** You ask 100 questions. You read 100 answers. (Takes all day).  \n",
    "- **Automated Evaluation:** You have an â€œAnswer Keyâ€. You write a script to check if the AIâ€™s answer matches the key.\n",
    "\n",
    "#### 2. Visual-in-Words: RAGAS (The Scorecard)  \n",
    "There is a standard framework called **RAGAS** (Retrieval-Augmented Generation Assessment). It measures:  \n",
    "\n",
    "- **Faithfulness:** Did the AI stick to the context? (No â€œMystariaâ€.)  \n",
    "- **Answer Relevance:** Did it actually answer the question? (No â€œI like turtles.â€)  \n",
    "- **Context Precision:** Did the retrieval find the right chunk?  \n",
    "\n",
    "Since installing RAGAS libraries can be heavy, we will build a **Simple Judge** using the LLM itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831f6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
